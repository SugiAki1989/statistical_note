---
title: "時系列データの分析(ARモデル)"
pagetitle: "時系列データの分析(ARモデル)"
output:
  html_document:
    toc: TRUE
    toc_depth: 5
    toc_float: FALSE
    # number_sections: TRUE
    code_folding: "show"
    highlight: "kate"
    # theme: "flatly"
    css: ../style.css
    md_extensions: -ascii_identifiers
---

```{r SETUP, include = FALSE}
knitr::opts_chunk$set(echo       = TRUE,
                      message    = FALSE,
                      warning    = FALSE,
                      #out.width  = 1280,
                      #out.height = 720,
                      # fig.dim = c(8, 6),
                      fig.align  = "center",
                      dev        = "ragg_png")
```

<div class="update-right">
UPDATE: `r Sys.time()`
</div>

# はじめに

時系列データへの理解、分析方法について、まとめていく。時系列データは、これまでやらないといけないと思いつつも、基礎をすっとばして、Prophet(一般化加法モデルベース)や状態空間モデルをやっていたが、やはり基礎は大事だと思うことがたくさんあったので、基礎からARIMAくらいまでをおさらいする。多分何回にわかれる。

## 時系列データとは
データが一定の間隔で順序を持っており、順序関係に意味を持っている場合、それは時系列データと考えられる(ティックデータとかの話は置いておく)。見るまでもないが下記のようなデータのこと。原系列ということもある。

```{r}
library(tidyverse)
set.seed(1989)

n <- 100
y <- vector(mode = "numeric", length = n)
y[1] <- 10
for (i in 2:length(y)) {
  y[i] <- 2 + 0.8*y[i - 1] + rnorm(1, mean = 0, sd = 1) 
}

df <- tibble::tibble(index = 1:n, y = y)

ggplot(data = df, aes(index, y)) + 
  geom_line(col = "#01489D") + 
  scale_y_continuous() +
  ggtitle("Time Series Data") + 
  theme_bw()
```


時系列データに対して、ピアソン相関係数を計算することが順序関係を考慮することをやめることを意味する。

## 時間の依存関係
時間の依存関係とは、過去の値と同じような大きな値がでやすいだとか、小さい値がでやすいだとかを意味する。視覚的に確認する場合は、`lag()`でレコードをずらして可視化すればいい。

```{r}
df2 <- df %>% 
  dplyr::mutate(lag1 = lag(y,1)) %>% 
  filter(!is.na(lag1))

ggplot(data = df2, aes(lag1, y)) + 
  geom_point(col = "#01489D") + 
  ggtitle("Lag1 Scatter Plot") + 
  theme_bw()
```

このデータは前の時点の値に関係するように作っているので、案の定、1時点ずらしたデータは同じような値がでやすくなっている。ちなみに、時間的な依存関係がないホワイトノイズを1時点ずらしたデータを可視化してみる。

```{r}
df3 <- 
  tibble::tibble(index = 1:n, noise = rnorm(n,0,1)) %>% 
  dplyr::mutate(lag1 = lag(noise,1)) %>% 
  filter(!is.na(lag1))

ggplot(data = df3, aes(noise, lag1)) + 
  geom_point(col = "#01489D") + 
  ggtitle("Lag1 Scatter Plot ~White Noise~") + 
  theme_bw()
```

このように、時間的な差を考慮した相関係数を**自己相関係数**という。どれくらいの時点をずらすかで、呼び名が代わり、先程の場合はラグ1の自己相関係数という。

```{r}
list(cor(df2$y, df2$lag1), cor(df3$noise, df3$lag1))
```

このずらした時点を連続して、n次の自己相関係数を可視化した図を**コレログラム**という。とりあえず30時点ずらしてみる。

```{r}
lags <- function(data, variable, n=10){
  variable <- rlang::enquo(variable)
  
  indices <- seq_len(n)
  quosures <- purrr::map( indices, ~rlang::quo(lag(!!variable, !!.x)) ) %>% 
    purrr::set_names(sprintf("lag_%02d", indices))
  
  mutate(data, !!!quosures )
}

df4 <- df %>% 
  dplyr::select(-index) %>% 
  lags(y, 30) %>% 
  dplyr::filter(!is.na(lag_30)) %>% 
  cor() %>% 
  tibble::as_tibble() %>% 
  dplyr::select(y) %>% 
  mutate(index = row_number())

ggplot2::ggplot(df4, aes(index, y)) + 
  geom_bar(stat = "identity", fill = "#01489D") + 
  ggtitle("Autocorrelation Function") + 
  theme_bw()
```

こんな感じでずらして計算すればいいんだけど、便利な関数が山ほどある。`acf()`を使えばいい。ラグ1からラグ10に向かって、自己相関が弱くなっていくのがわかる。つまり、過去に遡るほど弱くなる。

```{r}
# Autocorrelations of series ‘df$y’, by lag
# 
#      0      1      2      3      4      5      6      7      8      9     10 
#  1.000  0.756  0.575  0.484  0.409  0.345  0.251  0.185  0.085  0.001 -0.005 
#     11     12     13     14     15     16     17     18     19     20     21 
# -0.023 -0.099 -0.193 -0.230 -0.280 -0.330 -0.339 -0.360 -0.376 -0.325 -0.328 
#     22     23     24     25     26     27     28     29     30 
# -0.354 -0.373 -0.350 -0.338 -0.340 -0.299 -0.245 -0.216 -0.211 
```

他にも`forecast`パッケージの`ggtsdisplay()`なども便利。このプロット右下にあるのが、**偏自己相関係数**である。回帰分析の偏回帰係数みたいなもので、他の影響を差っ引いた該当変数間の影響関係を見るもの。つまり、ラグ3はラグ2の関係を受けるし、ラグ4はラグ3やラグ2の関係を受けてしまうためである。ちなみに青い線は95%信頼区間なので、これを超えると、自己相関ありと考える。p値の解釈は難しいので、Yes/Noと考えないように(自戒)。

```{r}
library(forecast)
forecast::ggtsdisplay(df$y)
# acf(x = df$y, lag.max = 30, type = "partial", plot = FALSE)
# 
# Partial autocorrelations of series ‘df$y’, by lag
# 
#      1      2      3      4      5      6      7      8      9     10     11 
#  0.756  0.009  0.109  0.015  0.015 -0.090  0.000 -0.141 -0.051  0.081 -0.019 
#     12     13     14     15     16     17     18     19     20     21     22 
# -0.137 -0.123 -0.009 -0.127 -0.071 -0.012 -0.075 -0.033  0.115 -0.164 -0.123 
#     23     24     25     26     27     28     29     30 
# -0.067 -0.014 -0.127 -0.053  0.003 -0.012 -0.045 -0.124 
```


自己相関を持つ時系列データかどうかは、Ljub-Box検定をすれば良い。帰無仮説は「この時系列データは、自己相関関係を持っていない」である。結果を見る限り、自己相関関係があるようである(そうデータを作ったので当然である)。

```{r}
 Box.test(x = df$y, type = "Ljung-Box")

```

## 弱定常性(Weak Stationarity)
時系列データの確率変数はどのように考えるのか。確率変数列$({R_{1},R_{2},R_{3},...R_{n}})$があるときに、その実現値を${r_{1},r_{2},r_{3},...r_{n}}$と定義する。つまり、ある時点$t $で、繰り返し観測した場合の1つの値がたまたま得られた値が$r_{t} $ということ。その値が、下記の仮定をもつ場合、**弱定常性**をもつという。

$$
\begin{eqnarray}
E(R_{t}) = a \\
VAR(R_{t}) = \gamma_{0} \\
Cov(R_{t}, R_{t-h}) = \gamma_{h}
\end{eqnarray}
$$

上から順にデータの平均と分散は一定で、1時点遡った値との関係も、1時点先の値との関係も、関係性の強さが変わらないということ。定常性の仮定を置くことで様々なモデルを組み立てられる。仮定あってのモデル。ちなみにホワイトノイズは下記の仮定をもつ。

$$
\begin{eqnarray}
E(U_{t}) = 0 \\
VAR(U_{t}) = \sigma^{2} \\
Cov(U_{t}, U_{t-h}) = 0
\end{eqnarray}
$$

## 自己回帰モデル(Autoregressive Model)
時点$t$に対して、1時点前までのデータ$y_{t-1}$を用いて回帰するARモデルを1次ARモデルと呼びます。AR(1)モデルともいう。パラメータは最小二乗法で求められる。AR(p)モデルの場合は未知パラメタが増えるが同じように求まるそう。

$$
\begin{eqnarray}
y_{t} = c + \phi_{1} y_{t-1} + \varepsilon_{t} , \varepsilon_{t} 〜 W.N.(\sigma^{2})
\end{eqnarray}
$$

なので、いつもどおり残差を考える。

$$
\begin{eqnarray}
e_{t} = y_{t} - \tilde{ c } + \tilde{ \phi_{1} } y_{t-1}
\end{eqnarray}
$$
この残差平方和を最小にするパラメタを求める。
$$
\begin{eqnarray}
SSR = \displaystyle \sum_{t=2}^T e^{2}_{t} = \sum_{t=2}^T ( y_{t} - \tilde{ c } + \tilde{ \phi_{1} } y_{t-1} )^{2}_{t}
\end{eqnarray}
$$
あとは最小になるよう0とおいて、偏微分する。
$$
\begin{eqnarray}
\frac{ \partial SSR }{ \partial  \tilde{ c }} = -2 \sum_{t=2}^T( y_{t} - \tilde{ c } + \tilde{ \phi_{1} } y_{t-1} )=0 \\
\frac{ \partial SSR }{ \partial  \tilde{ \phi_{1} }} = -2 \sum_{t=2}^T  y_{t-1}(  y_{t} - \tilde{ c } + \tilde{ \phi_{1} } y_{t-1} )=0 
\end{eqnarray}
$$
これを解く際に、ここで、
$$
\begin{eqnarray}
\bar{y}_{r, s} = (s - r - 1)^{-1} \sum_{t=r}^S y_{t}
\end{eqnarray}
$$
とおくと、下記の正規方程式が得られる。
$$
\begin{eqnarray}
\hat{ \phi_{1} } = \frac{\sum_{t=2}^T  (y_{t} - \bar{y}_{2, T}) (y_{t-1} - \bar{y}_{1, T-1})} {\sum_{t=2}^T  (y_{t-1} - \bar{y}_{2, T})^{2} } \\
\end{eqnarray}
$$
$$
\begin{eqnarray}
\bar{c} = \bar{y}_{2, T} - \hat{ \phi_{1} } \bar{y}_{1, T-1}
\end{eqnarray}
$$
というわけでRでやってみる。ちなみにこのデータの係数は0.8で作っている。

```{r}
# n <- 100
# y <- vector(mode = "numeric", length = n)
# y[1] <- 10
# for (i in 2:length(y)) {
#   y[i] <- 2 + 0.8*y[i - 1] + rnorm(1, mean = 0, sd = 1) 
# }
```

`ar()`の`aic`は`order.max`と組み合わせて、どのラグまでパラメタを考慮するのが良いかをAICだ最小になるモデルをもとに計算してくれる。`method`でパラメタの推定法が指定できる。

```{r}
ar(x = df$y, aic = FALSE, order.max = 1, method = "ols")

```

結果は0.7621ですこしずれているが、サンプルサイズを大きくすれば一致すると思う。

```{r}
set.seed(1989)
n <- 10000
y <- vector(mode = "numeric", length = n)
y[1] <- 10
for (i in 2:length(y)) {
  y[i] <- 2 + 0.8*y[i - 1] + rnorm(1, mean = 0, sd = 1) 
}

df <- tibble::tibble(index = 1:n, y = y)

ar(x = df$y, aic = FALSE, order.max = 1, method = "ols")

```

ちなみに自己相関関係を持っていないと回帰係数のパラメタが0に近くなるので、計算されない。

```{r}
df_wn <- tibble::tibble(index = 1:1000, y = rnorm(1000,0,1))
ar(x = df_wn$y, aic = TRUE, order.max = 1, method = "ols")

```
